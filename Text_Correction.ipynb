{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A2lAuQrUuofJ2ASkkBbHyWa9aNj2KOSL",
      "authorship_tag": "ABX9TyMqz4+dlxx0AX7rJiX2UNc5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amelbnmbh/Text-Correction-NLP-/blob/main/Text_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the Environment"
      ],
      "metadata": {
        "id": "CdVZ9yiY1p1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>The following files:\n",
        "\n",
        "`englishvoc.txt:` This file should contain a list of English words (the lexicon).\n",
        "\n",
        "`text.txt:` This is the file containing the text with spelling errors.\n",
        "\n",
        "`ref.txt:` This file is the corrected version of the text. If you don't have it yet, you can manually create it by correcting **text.txt**."
      ],
      "metadata": {
        "id": "XRlAFf4Z1uR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Install Required Libraries"
      ],
      "metadata": {
        "id": "UtDMwRa62O_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmhB1dmD2TGt",
        "outputId": "8302ff66-56e7-42a3-817f-19f7e8c2bc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.0)\n",
            "Requirement already satisfied: Levenshtein==0.26.0 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.0->python-Levenshtein) (3.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Import Required Libraries"
      ],
      "metadata": {
        "id": "9C-FQigGMGNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from Levenshtein import distance\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score"
      ],
      "metadata": {
        "id": "i4sPTnSZGGbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Download NLTK Stopwords"
      ],
      "metadata": {
        "id": "xL4ubYHjMJwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-oxdJ5VLgYj",
        "outputId": "fce4fa0c-99cf-43de-c0da-87badceb57aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Define File Reading Functions"
      ],
      "metadata": {
        "id": "PLUeddnr2SW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read text files\n",
        "def read_file(file_path):\n",
        "    \"\"\"Read a file and return its contents.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()"
      ],
      "metadata": {
        "id": "t7M9gJb43VaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the lexicon\n",
        "def read_vocab(file_path):\n",
        "    \"\"\"Read a vocabulary file and return its contents as a list of words.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read().splitlines()"
      ],
      "metadata": {
        "id": "w_ckosDa3pzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Define File Paths"
      ],
      "metadata": {
        "id": "86LWLB5gMYlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "text_file_path ='text.txt'\n",
        "vocab_file_path = 'englishvoc.txt'\n",
        "ref_file_path = 'ref.txt'"
      ],
      "metadata": {
        "id": "rzxbikQNKYCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Read the Files"
      ],
      "metadata": {
        "id": "6SjPjPR0MdpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the files\n",
        "text = read_file(text_file_path)\n",
        "vocab = read_vocab(vocab_file_path)\n",
        "ref = read_file(ref_file_path)"
      ],
      "metadata": {
        "id": "Trl2pNFHKgIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Levenshtein Distance Calculation"
      ],
      "metadata": {
        "id": "xG3d1kuB3ynp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Preprocess the Text"
      ],
      "metadata": {
        "id": "cahscAVhMwYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text"
      ],
      "metadata": {
        "id": "BSZtqNaE30EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Check Words Against Lexicon"
      ],
      "metadata": {
        "id": "-1ftAL2SM24A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_words(text, lexicon):\n",
        "    words = text.split()\n",
        "    correct_words = []\n",
        "    incorrect_words = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in lexicon:\n",
        "            correct_words.append(word)\n",
        "        else:\n",
        "            incorrect_words.append(word)\n",
        "\n",
        "    return correct_words, incorrect_words"
      ],
      "metadata": {
        "id": "1TqjH_rT3-74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Suggest Corrections for Incorrect Words"
      ],
      "metadata": {
        "id": "8Y_Ia97lNALK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_correction(word, lexicon):\n",
        "    min_distance = float('inf')\n",
        "    correction = word\n",
        "\n",
        "    for lex_word in lexicon:\n",
        "        lv_distance = distance(word, lex_word)\n",
        "        if lv_distance < min_distance:\n",
        "            min_distance = lv_distance\n",
        "            correction = lex_word\n",
        "\n",
        "    return correction"
      ],
      "metadata": {
        "id": "4c9UV6keK7yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Preprocess and Check Words"
      ],
      "metadata": {
        "id": "4lMrc9iXNGrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = preprocess_text(text)\n",
        "correct_words, incorrect_words = check_words(preprocessed_text, set(vocab))"
      ],
      "metadata": {
        "id": "2xuFnHeg4BvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Correct Incorrect Words"
      ],
      "metadata": {
        "id": "vSHgdDVSNLgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_words = []\n",
        "for word in incorrect_words:\n",
        "    corrected_word = suggest_correction(word, set(vocab))\n",
        "    corrected_words.append(corrected_word)\n",
        "\n",
        "# Final corrected text\n",
        "corrected_text = ' '.join(correct_words + corrected_words)"
      ],
      "metadata": {
        "id": "VBwmqcNANPqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Evaluate the Correction"
      ],
      "metadata": {
        "id": "WbmreXNdNWTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_words = corrected_text.split()\n",
        "reference_words = ref.split()\n",
        "\n",
        "# Create binary labels (1 for match, 0 for no match)\n",
        "y_true = [1 if word in reference_words else 0 for word in corrected_words]\n",
        "y_pred = [1 if word in corrected_words else 0 for word in corrected_words]\n",
        "\n",
        "# Calculate precision, recall, and accuracy\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "fvRGaLCiNXum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "RKnusDggNc9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cVCFtZRNeTz",
        "outputId": "df4c7978-289f-4a62-8072-b5cd7746fc1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6202046035805626\n",
            "Recall: 1.0\n",
            "Accuracy: 0.6202046035805626\n"
          ]
        }
      ]
    }
  ]
}